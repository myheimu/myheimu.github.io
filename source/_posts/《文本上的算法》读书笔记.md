---
title: 《文本上的算法——深入浅出自然语言处理》读书笔记
date: 2019-04-27
updated: 2019-05-06
categories:
- NLP
tags:
- 读书笔记
permalink: algorithm-for-text-note
mathjax: true
---

这本书说是自然语言处理，实际上主要是一些数学基础、机器学习基础，以及搜索、推荐、广告、对话等各方面的介绍。

全书结构方面不太好，各处顺序有些奇怪，经常在一大块内容之后又带上好多零碎内容不好归类。

内容方面，涉及较广，内容蛮新，内容也有深度。不过整体上是博客风格，有的分析比较深，有的一笔带过，各不相同。

作者很多地方更强调模型之间的发展逻辑和对比关系，视角更高，了解更深，这一点非常棒。

作为入门阅读的话，要忽略掉一些困难的推理以及论文综述的每篇论文内部思路，重点了解整体内容，不要陷入推导和前沿论文的沼泽。

作为进阶阅读的话，需要多查询其他资料，尽可能把各个部分都做到彻底吃透（事实上作者也有些逃避，很多内容只挑简单部分说而对复杂部分一笔带过），建议数学部分需要全部吃透，综述部分可以挑自己感兴趣的领域，虽然这很困难。

PS：为了记录方便，对于公式采用简化的纯文本写法，简单的上下标用unicode符号直接表示，复杂的用LaTex写法（正常文本_{下标}，正常文本^{上标}），∑、∏等不详细写范围。准确公式请参照原文或查询资料。

# 第1章 一些基础知识
信息论：信息熵、联合熵、条件熵、相对熵（KL散度、信息增益）、互信息、交叉熵

概率论、贝叶斯法则：P(x|y) = (P(y|x)P(x)) / P(y)

先验概率（Prior probability）：P(x)，“因”的固有常识、经验概率

后验概率（Posterior probability）：P(x|y)，通过“果”来推断“因”的条件概率

似然函数（Likelihood function）：𝓛(y|x) = C×P(y|x)，由“因”导致“果”的可能性

（ 对此查了一些资料，总结为：https://zhuanlan.zhihu.com/p/63485232 ）

# 第2章 寻求最优解
几乎所有机器学习问题背后都是最优化问题，即：min 𝑓(x)  s.t. ℎ(x)=0, 𝑔(x) ⩽ 0
* 没有约束条件ℎ和𝑔的叫无约束优化，只有ℎ的叫有等式约束优化，ℎ和𝑔都有的叫有不等式约束优化。

目标函数（损失函数）：常见的有平方损失、绝对损失、合页损失、似然损失等

正则化：L₀范数、L₁范数、L₂范数

损失函数的期望最小化（经验风险最小化）+正则化最小化（结构风险最小化）

优先选择熟悉的模型。如果模型了解非常透彻只需要代入数据，则叫参数估计；如果不够了解，则叫非参数估计。

最大似然估计（概率派观点）
* θ_{MLE} = argmax_{θ} p(x|θ) = argmax_{θ} ∏ P(xᵢ|θ)
* 换用对数形式，即 θ_{MLE} = argmax_{θ} ∑ log(p(xᵢ|θ))

最大后验估计（贝叶斯派观点）
* θ_{MAP} = argmax_{θ} p(x|θ)p(θ)

最小二乘法估计（仅针对高斯分布）
* 使得平方和损失最小，解析解

无约束优化问题，可以采用梯度下降、牛顿法/拟牛顿法、共轭梯度法等。
* 梯度下降：批量梯度下降、随机梯度下降SGD、mini-batch梯度下降

有约束优化问题，大多用拉格朗日乘子法转化为无约束优化问题求解。
* 等式约束优化问题，拉格朗日乘子法：L(x,α) = f(x) + αh(x)
* 不等式约束优化问题，拉格朗日乘子法：L(x,α,β) = f(x) + αh(x) + βg(x)  (β>=0)
* 原问题不好解决情况下，可以采用对偶问题

# 第3章 让机器像人一样学习
监督学习的两种模型
* 判别式模型，其概率图是无向图，求解条件概率p(y|x)。只关心类的边界。例如LR、SVM、NN、CRF等
* 生成式模型，其概率图是有向图，求解概率p(x|y)和p(y)，然后根据贝叶斯法则求后验概率。关心每个类的具体分布。例如朴素贝叶斯、HMM、贝叶斯网络、LDA等。

## 3.2 逻辑回归/因子分解机
**线性回归模型** y = f(x) = θᵀx

**逻辑回归模型LR**，在线性回归基础上加上sigmoid —— g(z)=1/(1+e⁻ᶻ^{-z})，则最终h_θ(x) = g(θᵀx)

如果采用平方损失作为目标函数，由于sigmoid存在，其目标函数并非凸函数，没有最优解。

因此采用最大似然估计：
* 针对二分类，假设p(y=1|x,θ) = h_θ(x)，那么p(y=0|x,θ) = 1 - h_θ(x)
* 概率分布p(y|x,θ) = (h_θ(x))ʸ (1 - h_θ(x))¹⁻ʸ
* 似然函数𝓛(θ) = p(Y|X, θ) = ∏ p(yʲ|xʲ,θ)
* 取log化，L(θ) = log𝓛(θ)
* L(θ)对θᵢ求导，得到(1/M) ∑ (yʲ - h_θ(xʲ)) xᵢʲ
* 随后对其进行梯度上升法。（公式刚好与线性回归是一样的）

把逻辑回归扩展到多分类，也就是使用softmax：
* p(y=c|x,θ_c) = softmax(θ_cᵀx)
* 同样也可以通过最大似然估计，计算出梯度上升法公式。

**因子分解机FM**

考虑特征关联性，进行两两组合 y(x)=w₀+∑wᵢxᵢ + ∑∑wᵢⱼxᵢxⱼ

但样本稀疏情况下很难训练

针对wᵢⱼ组成的对称矩阵尝试分解 W = VᵀV，此时wᵢⱼ=<vᵢ,vⱼ> （点积）

然后可以推导y(x)到相关参数wᵢ和vⱼ,ₖ的偏导，用于梯度上升计算

延伸到在线计算，FTRL

## 3.3 最大熵模型/条件随机场
最大熵模型
* 特征函数f(x,y)，当x,y为样本中某个xᵢ,yᵢ时为1，否则为0
* 样本期望p᠆(f) = ∑ p᠆(x,y) f(x,y)
* 模型期望p(f) = ∑p(x,y)f(x,y) = ∑ p᠆(x) p(y|x) f(x,y)
* 目标函数：X情况下Y的条件熵：argmaxₚH(p) = -∑ p᠆(x) p(y|x) log p(y|x)
* 限制条件1：∀fᵢ ∑ p᠆(x,y) fᵢ(x,y) = ∑ p᠆(x) p(y|x) fᵢ(x,y)
* 限制条件2：∀x ∑ p(y|x) = 1
* 采用拉格朗日乘子法，其中参数采用一系列a，直接求导为零难以计算
* 采用对偶原理，将minₚmaxₐ L(p, a)转换为maxₐminₚ L(p, a)
* 计算得到p(y|x)解析解
* 然而a难以继续解析求解，需要采用GIS、IIS、LBFGS等数值求解方法
* 例如IIS，利用最大似然估计L'(a) = ∑p᠆(x,y) log p(y|x) 对a进行数值迭代求解

条件随机场CRF简单提及，具有类似的方法，只不过概率图上会利用上下文

## 3.4 主题模型 
PLSA（Probabilistic Latent Semantic Analysis）
* d∈D表示文档，w∈V表示词语，z表示隐含主题
* P(dᵢ)表示单词在文档dᵢ中概率，P(zₖ|dᵢ)表示主题 zₖ在给定文档dᵢ下出现的概率，P(wⱼ|zₖ)表示单词 wⱼ在给定主题 zₖ下出现的概率
* P(dᵢ, wⱼ) = P(dᵢ) P(wⱼ|dᵢ) = P(dᵢ)  ∑  P(wⱼ|zₖ) P(zₖ|dᵢ)
* 由于词与词之间是相互独立的，文档与文档之间也是相互独立的，则整个样本集的分布为：（n表示词在文档中的次数）
P(D,V) =  ∏ ∏ (P(dᵢ, wⱼ)^n( wⱼ, dᵢ))
* 待估计的参数为θ = { P(wⱼ|zₖ),  P(zₖ|dᵢ) }
* 样本集的log似然函数：
L(θ) = log P(D,V|θ) = ∑ᵢ∑ⱼ n(wⱼ, dᵢ) log P(dᵢ, wⱼ)


EM算法
* 对参数θ = { P(wⱼ|zₖ),  P(zₖ|dᵢ) }赋随机值
* 第一步E-step：求后验概率
P(zₖ|dᵢ,wⱼ) = (P(wⱼ|zₖ)P(zₖ|dᵢ)) / (∑ₖP(wⱼ|zₖ)P(zₖ|dᵢ))
* 第二步M-step：最大化似然函数的下限，解出新的参数
给定下限 Q(θ) = ∑ᵢ∑ⱼ n(wⱼ, dᵢ) ∑ₖP(zₖ|dᵢ,wⱼ)log(P(wⱼ|zₖ)P(zₖ|dᵢ)) ≤ L(θ)
* 外加约束条件 ∑ⱼP(wⱼ|zₖ)=1和 ∑ₖP(zₖ|dᵢ)=1
* 采用拉格朗日乘子，随后对P(wⱼ|zₖ)和P(zₖ|dᵢ)求导并得到新值
* 把新值带入第一步E-step继续迭代


LDA（Latent Dirichlet Allocation）
* 相比PLSA增加了对参数的先验分布，从而避免过拟合
* 假设了基于超参数的Dirichlet分布，以及与之共轭的Multinomial分布做下一步的先验分布
* 由于LDA相比PLSA无法求解后验分布，所以要使用变分-EM算法，或者采用Gibbs Sampling算法

Gibbs Sampling算法是MCMC的一个特例，如果概率不易求得，可以交替地固定某一维度，然后通过其他维度值进行抽样近似求解

（书上有详细的讲解，还有配套代码，参考文献如下）
* 《Variational Message Passing and Its Applications》
* 《Latent Dirichlet Allocation》
* 《Parameter Estimation for Text Analysis》
* 《The Expectation Maximization Algorithm A Short Tutorial》
* 《Gibbs Sampling in The Generative Model of Latent Dirichlet Allocation》

## 3.5 深度学习
BP神经网络（1层隐藏层），及其向后传播的梯度推导过程

浅层网络表达能力不强，而深层网络存在训练困难，因此早期采用贪婪式逐层学习方法

### 3.5.3 词表示（word embedding）
NNLM（神经网络语言模型）
* 用前n-1个词，查询词向量，然后拼接：x = (C(w_{t-n+1}, ..., C(w_{t-1})))
* 网络结构：y = b + W·x + U·tanh(d + Hx)
* 最后softmax，即P(w_t|w_{t-n+1}, ..., w_{t-1}) = softmax(y_{w_t})
* 参数为θ = (b,d,W,U,H,C)

NNLM一种改进LBL（Log-Bilinear LM）
* 对前n-1个词，查询词向量，然后乘以矩阵并加和：h_{wₙ} = ∑ Hᵢ × C(wᵢ)
* 针对不同的词w_i，计算内积：y = C(wᵢ)ᵀ h_{wₙ}
* 最后对y进行softmax

RNNLM（循环神经网络语言模型）
* 循环神经网络，w(t)为词w_t的one-hot向量，s(t-1)为上次状态，U为embedding：s(t) = sigmoid(U·w(t) + W·s(t-1))
* 输出：P(w_{t+1} | w_t, s(t-1)) = y(t) = softmax(V·s(t))

Word2Vec模型
* CBOW 给定一个词的窗口上下文，预测该词
* skip-gram 给定一个词，预测上下文
* Hierarchical softmax方法减少计算量
* Negative sampling方法减少计算量

解决多义词方法（讲的不清楚，考虑近年ELMo、BERT已经优美解决，所以没有详细看）
* 使用Topic Model得到每个词在每个类别的概率，然后融合到词表示模型中，但对长尾效果不好
* 词聚类，把每个词根据上下文归入某一类，然后融合到词表示模型中，但对长尾效果不好
* 实现挖掘各个词属于各个类别的先验概率，举了一种Word Multi-Embedding的方法

词表示用法：
* 把词表示当成一种额外特征补充引入模型
* 把词表示作为直接输入完成一些nlp任务，见《Natural Language Processing (Almost) from Scratch》


### 3.5.4 句子表示（简单介绍没有展开）
* 方法一：直接将词向量累加平均
* 方法二：2-gram进行卷积，最终平均池化
* 方法三：结合句法分析，ReNN（递归神经网络）方式生成句子表示
* 方法四：计算句向量时在每个句子前面赋予唯一的id，当训练结束后，id的向量就可以当做句向量。但无法处理新句。（《Distributed Representations of Sentences and Documents》）
* 方法五：把句子看成词，直接训练。但无法处理新句。（《A Model of Coherence Based on Distributed Sentence Representation》）
* 方法六：使用RNN将句子转变为向量，用它预测前后句子，使得概率最大。可以预测新句（《Skip-Thought Vectors》）

### 3.5.5 深度学习常见模型
针对RNN任务，通常使用交叉熵损失函数，其中真实标签是one-hot的，则其损失函数等价于对数似然函数：

E = -(1/T)  ∑ log(ỹ_{t,j})  其中j表示t时刻真实词的位置

梯度下降的优化算法：（简单列了公式，需要查看更多资料）
* 动量法：为当前迭代更新中，加入上一次的迭代更新
* AdaGrad：自适应地为各个参数分配不同学习率
* AdaDelta：用一阶方法近似模拟二阶牛顿法
* Rmsprop：在mini-batch学习上效果不错的算法


RNN 循环神经网络
* hₜ = δ (W×xₜ + U×hₜ₋₁)  // 隐藏层，传递状态
* ỹₜ = f(V×hₜ)  // 输出层
* 采用BPTT（基于时间的反向传播）进行训练，容易出现梯度爆炸和梯度消失问题。Mikolov提出对梯度爆炸的裁剪。梯度消失需要更好的模型解决。

LSTM：一种基于门控制的优化RNN
* 遗忘门（相应公式略，下同）
* 输出门
* 输入们
* 隐藏层传递C_t和h_t两个状态

GRU：一种比LSTM更简单的基于门控制的优化RNN
* 更新门
* 重置门
* 隐藏层只传递h_t状态

CW-RNN（Clockwork RNN） （没有深入了解）
* 将神经元分组，并按组分配时钟周期T_i，每个组内神经元全连接
* 时钟变化过程中，只有满足(t mod T_i) = 0的隐藏层组才会被执行

以上几类RNN变种，其实都差不多——《LSTM: A Search Space Odyssey》

Attention
* 避免传统encoder-decoder架构中通过一个固定长度状态变量来传输全部信息的信息损失问题
* 利用状态变量+上下文注意力信息
* 最早提出：《Neural Machine Translation by Jointly Learning to Align and Translate》
* 本书作者更倾向于在模型中加入先验信息的优化，比如《Contextural LSTM Models for Large Scale NLP Task》


深度学习模型训练技巧：
* 正则化：L1/L2
* Dropout
* 增加验证集。如果验证集上效果变差，就提前停止训练
* 增加并仔细筛选训练数据
* 权重初始化的选择
* 各种调参：batch大小、各层神经元数量、dropout比例、学习速率等
* 激活函数的选择，ReLU
* 文本任务中最后一步词汇量太大的话softmax计算量会很大，可以优化
    * Hierarchical softmax （word2vec相关论文提及）
    * Noise Contrastive Estimation （word2vec相关论文提及）
    * Approximate softmax （需要额外找资料理解）


CNN 卷积神经网络
* 卷积层和池化层（也叫下采样层）
* CNN在图像上取得很好结果，也有很多时候用于文本


GAN 生成式对抗网络
* 生成网络G和判别网络D，轮流训练
* G的目标是去尽量生成真实图片欺骗D
* D的目标是尽量把G生成的图片和真实图片区分开
* 目前在文本上并无明显作用，但可能对于生成式对话未来有提升


### 3.5.5.6 深度学习的一些应用
对话模型（简单罗列）
* 使用seq2seq的机器翻译思路做对话《A Neural Conversational Model》《Neural Responding Machine for Short-Text Conversation》
* 结合上下文信息《Attention with Intention for a Neural Network Conversation Model》《A Neural Network Approach to Context-Sensitive Generation of Conversational Responses》
    * 在encoder和decoder之外建立intention network，用来传递前轮状态
    * 也可以把上下文c与当前输入m通过拼接，拼接后嵌入，或者分别嵌入等方式进入模型
* 采用MMI（最大互信息）作为目标函数提高多样性：《A Diversity-Promoting Objective Function for Neural Conversation Models》
* 采用Pointwise Mutial Information预测名词，然后用两个网络分别生成名词前半部分和后半部分，避免泛泛而谈《Sequence to Backward and Forward Sequences: A Content-Introducing Approach to Generative Short-Text Conversation》
* 引入增强学习，奖励非高频、不重复、互信息更好相关性《Deep Reinforcement Learning for Dialogue Generation》
* 引入用户属性《A Persona-Based Neural Conversation Model》
* 引入知识《Incorporating Unstructured Textual Knowledge Sources into Neural Dialogue System》
* 引入在线反馈《On-line Active Reward Learning for Policy Optimization in Spoken Dialogue Systems》

阅读理解模型（简单罗列）
* 主流方案是IR信息检索，从大量文档中检索出候选集，然后通过NLP技术挑选最佳答案。《Deep Unordered Composition Rivals Syntactic Methods for Text Classification》
* Facebook《Memory Network》依然采用找到候选集再挑选的方法，但采用神经网络表示候选集并抽取答案。
* 《End-To-End Memory Networks》
* 《Teaching Machines to Read and Comprehend》
* 《Text Understanding with The Attention Sum Reader Network》

匹配模型（简单罗列）
* 对候选集进行匹配，通常是把question和answer分别放入RNN或CNN形成向量，最后计算余弦相似度。
* 《The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems》
* 《Deep Learning for Answer Sentence Selection: A Study and An Oten Task》
* 《LSTM-Based Deep Learning Models for Non-factoid Answer Selection》
* 《ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs》
* 基于结构化知识三元组作为answer进行训练《Open Question Answering with Weakly Supervised Embedding Models》
* 将问题和问题中实体关联的子图分别表示成向量来计算《Question Answering with Subgraph Embeddings》


## 3.6 其他模型
kNN（k最近邻）：监督学习分类算法
k-means：无监督聚类

DT（Decision Tree，决策树）：确定分支时ID3算法选择信息增益最大的特征属性，C4.5算法则采用信息增益比

Bagging：典型的集成学习模型。有放回采样N个训练集，然后训练N个弱分类器，再多数投票。
* 随机森林：对Bagging的升级版，随机选择一部分特征来做决策树的分支划分。

Boosting：典型的集成学习模型。串行式训练多个弱分类器，后一个分类器取决于前一个分类器分错的样本。
* AdaBoost，前一个分类器分对的样本选中概率降低，分错的选中概率提高

GBDT（Gradient Boosting Decision Tree）（写的非常简单，需要补充相关资料理解）
* Gradient Boosting是基于上一次弱模型的残差，进行下一次新的弱模型训练。
* GBDT中的决策树采用回归树，而非分类树
* GBDT可以直接用于分类或回归问题，也可以将叶子节点构成的向量作为新特征用于后续训练

SVM （写的简单，需要补充相关资料理解）
* 最优化分割间隔
* 通过核函数映射到高维空间，然后线性划分
* 核函数不需要真正映射，只需要能够快速算出映射到高维空间之后的向量内积即可，从而降低计算复杂度
* 最后采用SMO算法求解最优化模型（凸二次规划问题）

模型评价方法：
* 准确率、召回率、F1
* PR曲线、ROC曲线、AUC面积

# 第4章 如何计算得更快
算法优化、代码优化、分布式（hadoop）

# 第5章 搜索引擎相关术语
tf词频率，df词在文档的频率，idf逆文档频率 idfᵢ = log(|D| / dfᵢ)

IG（Information Gain，信息增益）：特征tᵢ所带来的信息增益IG(tᵢ)=Cⱼ的信息熵 - tᵢ条件下Cⱼ的条件熵

CHI（𝛘²统计量，卡方检验）：特征tᵢ与类别Cⱼ的相关联程度 𝛘²(tᵢ,Cⱼ)=∑((O-T)²/T)，采用四格表计算，随后对tᵢ与所有类别的𝛘²求最大值或者平均值作为全局性结果

MI（Mutial Information，互信息）：MI(tᵢ,Cⱼ)=log(P(tᵢ,Cⱼ)/P(tᵢ)/P(Cⱼ)) = log(P(tᵢ|Cⱼ) / P(tᵢ))，同上可以对tᵢ进行全局性计算

PageRank：根据投票原则，通过外链进行权重传递，最终收敛。《Deeper Inside PageRank》

相似度计算
* 距离方法：欧氏距离L₂范数、曼哈顿距离L₁范数、明氏距离、汉明距离、Jaccard相似系数、Jaccard距离、余弦距离、皮尔森相关系数、编辑距离等
* Hash方法：minhash（两个集合经随机转换后得到的两个最小hash值相等的概率等于两集合jaccard相似度）、simhash（使用simhash后的海明距离计算相似度）
文本中的相似度模型
* Bool模型：用词典数量为长度的向量表示，句子中存在某词，就标1，否则为0，然后计算两向量的余弦相似度
* tf*idf模型（增加词权重特征）：对Bool模型中词对应位置换用tf*idf数值
* BM25模型（增加长度特征）：增加长度相关归一化，避免长句占优
* Proximity模型（增加位置特征）：考虑位置信息
    * 《Term Proximity Scoring for Keyword-Based Retrieval Systems》
* 语义特征模型（增加Topic主题特征）：Term相关性（同上）+Topic相关性，其中Topic相关性可以使用语言模型（没说清），或者基于主题分布的余弦相似度，或者基于主题分布的KL距离（为保证对称性，累加正反两个KL距离）
    * 《Learning th Similarity of Documents: An Information-Geometric Approach to Document Retrieval and Categorization》
    * 《LDA Based Similarity Modeling for Question Answering》
    * 《Regularizing Ad Hoc Retrieval Scores》
* 句法特征模型（增加句法特征）：考虑句法树匹配程度，避免词类似但句法导致的含义巨变
* 深度表示模型（增加语义特征）：
    * DSSM（见过，所以不详细看了）
    * 基于ReNN，针对句法分析树进行计算，形成句子表示，然后类似DSSM进行query-doc的相似度计算（没有写论文，难道是作者的业务经验？）
    * 基于LSTM生成句子向量，后续同上

# 第6章 搜索引擎
## 6.1 搜索引擎原理
假设Q为搜索词Query，Dᵢ为所有网页集合中第i个网页，P(Dᵢ|Q)表示给定Q时第i个网页满足用户需求的概率。

概率语言模型：使用贝叶斯公式 P(Dᵢ|Q) = P(Q|Dᵢ) P(Dᵢ) / P(Q)
* P(Q)，对于同一个Query，它可视为无需计算的常数；但对不同Query来说，越常见的Query，分母越大，则需要更大的分子，即更好的结果。
* P(Dᵢ)，网页重要度，例如PageRank
*  P(Q|Dᵢ)，给定网页满足需求时，查询Q的概率
   *  可以假设Query中各词独立，转为P(t₁|Dᵢ)P(t₂|Dᵢ)...P(tₙ|Dᵢ)，形成词的倒排索引
然而实际使用中，直接获得P(Dᵢ|Q)可能比上述方法更好。可以依赖点击反馈直接获得，当然还要面临开发和探索问题（Exploitation&Exploration）

## 6.2 搜索引擎架构
线下模块
* 爬虫抓取
* 页面解析
* 权重计算
* 建立索引

线上模块
* 主控MC
* Query分析模块QA
* 基础搜索模块BS，从索引库中读取索引并归并完成初次筛选，基础相关性进行二次筛选
* 返回主控，进行高端排序AR，即rerank
* 最后从摘要库AD获得摘要返回给用户


## 6.3 搜索引擎核心模块
爬虫部分：深度或广度优先，遍历全网

网页解析：格式转换、html标签处理、解析标题和正文等

权重计算：页面不同部分term权重，语义特征，时间因子，文档质量，网页主题，PageRank等

索引生成：基于term的倒排索引，索引扩展，高低质量分库，水平分库

Query分析：
* term级别分析：分词、专名识别、term重要性、term紧密度
* query级别分析：意图识别（模板匹配、分类、点击反馈）、时效性判断、query变换（同义改写、纠错改写、省略变换）

主控模块MC：整体调度

基础检索BS：从索引库中进行一次过滤（简单模型比如BM25），一部分进行二次过滤（term权重、位置信息、语义信息、文档质量等）

高端排序AR：页面质量调权、query需求调权、时效性调权、多样性调权、权威性调权、点击调权。
* 点击模型《A Dynamic Bayesian Network Click Model for Web Search Ranking》
* 多样性《Diversifying Search Results》

Learning To Rank
* Pointwise：将排序问题转变为多分类问题或者回归问题，直接算出query下各文档得分。例如McRank算法。CTR预估通常采用pointwise。
* Pairwise：将排序问题转变为二元分类问题，针对同一query，计算两个doc之间的前后顺序。算法包括RankSVM、RankNet、Frank、RankBoost、GBRank等。
* Listwise：直接对排序指标（例如NDCG）进行优化。算法包括ListNet、SVM-MAP、LambdaRank、LambdaMART等。
* 商业搜索引擎中通常LTR并非端到端使用，只是用来计算一些高级特征供上层排序使用。（此处没有详细展开）

RankNet 《Learning to Rank using Gradient Descent》
* 对于URL对{Uᵢ, Uⱼ}，模型输出得分为sᵢ和sⱼ
* Uᵢ 比 Uⱼ 更相关的真实概率（标注数据）为Ṗᵢⱼ = (1+Sᵢⱼ)/2，其中Sᵢⱼ在Uᵢ 比 Uⱼ 更相关时为1，反向为-1，相关性相同为0
* Uᵢ 比 Uⱼ 更相关的预测概率（模型输出）为Pᵢⱼ = 1 / (1 + e^{-σ(sᵢ-sⱼ)})
* 采用两者交叉熵：Cᵢⱼ = -Ṗᵢⱼ log Pᵢⱼ - (1-Ṗᵢⱼ) log(1-Pᵢⱼ)，总计所有pair：C = ∑ Cᵢⱼ
* 梯度下降，对模型参数进行训练
* 可以限制pair对为有序对（即Uᵢ > Uⱼ），则可大大简化公式

LambdaRank
* 相比RankNet仅优化错误对，更加关注NDCG
* NDCG非平滑不连续，无法直接求梯度，所以直接定义经验梯度：
* λᵢⱼ = ∂C(sᵢ-sⱼ) / ∂sᵢ = - σ / (1+e^{-σ(sᵢ-sⱼ)}) × |ΔNDCG|
* 一部分包括RankNet中的梯度，一部分包括交换Uᵢ、Uⱼ位置后NDCG的变化差值

LambdaMART
* 基于LambdaRank的梯度，加上MART（GBDT）形成
* 具体演进《From RankNet to LambdaRank to LambdaMART: An Overview》

训练特征：query信息、文档信息、匹配信息、语义信息、用户信息等

索引归并算法：
* Term-At-A-Time(TAAT)：每次只打开一条倒排链，对单个term的列表进行遍历
* Document-At-A-Time(DAAT)：同时对多倒排链进行归并，完整处理文档所有term
    * 可以采用跳表来优化《Skip Lists: a Probabilistic Alternative to Balanced Trees》

查询表达式
* 常用的包括AND、OR、NOT
* 跳表可以优化AND
* 但对于OR就需要剪枝限界，例如MaxScore和Weak-AND算法，基于估算相关性上限来提前结束 《Efficient Query Evaluation using a Two-Level Retrieval Process》
* 也可以基于term的重要度，进行内部and和or操作，对权重低的term进行提前剪枝

搜索引擎评价
* MAP（Mean Average Precision）：对准确的网页，按照位置调整权重并平均
* DCG（Discounted Cumulative Gain）：评价权重+位置权重，其与理想值IDCG的比值即NDCG
* MRR（Mean Reciprocal Rank）：第一个准确答案的排名情况，多用于导航和问答类评价

搜索引擎更多论文《Ranking Relevance in Yahoo Search》《Newral Models for INformation Retrieval》

搜索引擎其他模块：
* 输入提示
* 相关搜索
* 反作弊、反垃圾
* 网页驱虫
* 缓存
* 日志挖掘Query与Query关系乃至term之间关系等 《SimRank: A Measure of Structural-Context Similarity》

## 6.4 搜索广告
术语：CPC按每次点击收费，CPM按每千次展现收费，CPA按每次行动收费，CPS按销售收费，CPL按每次引导收费，CPD按天收费，CTR点击率，CVR转化率，RPM每千次展示收入

RTB（Real-Time Bidding，实时竞价）广告
* Ad Exchange 广告交易平台
* DSP 需求方平台
* SSP 供应方平台
* DMP 数据管理平台
* 针对每一次访问，根据用户的个性化标签，由各家广告主对此类用户的报价进行竞价

广告系统框架（简单介绍）
* Query分析
* 触发系统，根据query召回拍卖词，去广告库寻找广告
* 排序模块，广告质量Q×出价Bid，其中广告质量部分通常使用pCTR（预估点击率）
* 排序结果进行展现，对应触发计费系统。一般使用广义二阶价格。

# 第7章 如何让机器猜的更准（推荐）
## 基于协同过滤的推荐（Collaborative Filtering， CF）
User-Based CF
* p(u,i) = ȓᵤ + ∑ (sim(u,v)×(rᵥᵢ - ȓᵥ)) / ∑ sim(u,v)
* p(u,i)表示用户u对物品i的打分预测值，sim(u,v)表示用户u和用户v的皮尔森相关系数，rᵥᵢ表示用户v对物品i的打分， ȓᵤ表示用户u的平均打分

Item-Based CF
* p(u,i) = ∑ (sim(j,i)×rᵤⱼ) / ∑ sim(j,i)

对比来说，Item-Based CF可以进行线下物品计算，减少线上计算量，用的更多。

Slope One预测器《Slope One Predictors for Online Rating-Based Collaborative Filtering》

CF的大规模计划改造《Google News Personalization: Scalable Online Collaborative Filtering》

也可以基于SVD进行矩阵分解来做协同过滤《Application of Dimensionality Reducation in Recommender System - A Case Study》

## 基于内容的推荐
可以基于内容的分类、聚类，来给用户推荐

可以基于搜索，采用用户兴趣和用户上下文作为Query来进行搜索、排序
《Up Next: Retrieval Methods for Large Scale Related Video Suggestion》

## 混合推荐：真实的推荐系统架构（简单罗列）
用户画像

排序模型：《Deep Neural Networks for Youtube Recommendations》《Wide&Deep Learning for Recommender Systems》

物品属性计算：《Personalized News Recommendation Based on Click Behavior》《Personalized Recommendation on Dynamic Content Using Predictive Bilinear Models》《A Contextural-Bandit Approach to Personalized News Article Recommendation》

推荐系统评价：CTR、点击次数、多样性、新颖性等等。《Recommender System Performance Evaluation and Prediction: An Information Retrieval Perspective》《Evaluating the Accuracy and Utility of Recommender Systems》《Evaluating Recommendation Systems》

# 第8章 理解语言有多难
对NLP的一个分类：
* 资源：知识库（知识抽取、知识表示、知识推理）、词典（Log挖掘、Click挖掘、Session挖掘、Web挖掘）
* 词汇级别：词法层面分析（分词、词性标注、专名识别、新词发现）、词汇关系（Ontology构建、词聚类）
* 句子级别：语义层面分析（中心词提取、意图识别、Topic分析、语义分析）、变换层面分析（同义改写、纠错改写）、结构层面分析（组块分析、句法分析）
* 任务：文档分类/聚类、情感分析、机器翻译、摘要提取、搜索引擎/推荐系统、自动问答/人机对话

专名识别
* CRF序列标注
  * BiLSTM-CRF《Neural Architectures for Named Entity Recognition》
  * 人名、地名、时间词比较好识别，但音乐、小说等边界比较宽泛，必须搭配词表


**句法分析**

依存句法分析有两大类方法：
* 基于图的方法：有向图中最大生成树求解问题
* 基于策略的方法：局部贪心的序列动作，例如arc-standard和arc-eager

近年基于深度学习的依存句法分析，例如《A Fast and Accurate Dependency Parser using Neural Networks》（简单介绍）

**语义分析**

将自然语言转化为逻辑表达式
* 《semantic parsing via paraphrasing》
* 《Semantic Parsing on Freebase from Question-Answer Pairs》
* 《Large-scale Semantic Parsing via Schema Matching and Lexicon Extension》
* 《Semantic Parsing via Staged Query Graph Generation: Question Answering with Knowledge Base》
* 《SLING: A framework for frame semantic parsing》

**知识库**

无结构化知识和结构化知识（SPO三元组）
* 基于模板的抽取
* 基于统计模型的抽取


* 《End-to-end Relation Extraction using LSTMs on Sequences and Tree Structures》
* 《Language to Logical Form with Neural Attention》
* 《Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction》
* 《Representing Text for Joint Embedding of Text and Knowledge Bases》
《Multilingual Relation Extraction using Compositional Universal Schema》

**对话系统**（整体讲的比较概括，而本身又非常复杂，需要额外找资料了解）

问答型对话
* 基于语义分析的方法
* 基于信息抽取的方法
* 端到端的方法

任务型对话
* 系统记录状态和相应动作
* DST（Dialog State Tracking）用于获得当前状态
    * 生成式模型
    * 判别式模型
    * 规则系统
* Dialog Policy用于根据当前状态做出决策动作

闲聊型对话
* 规则方法
* 生成方法
* 检索方法