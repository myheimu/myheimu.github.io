---
title: 《程序员的数学2：概率统计》笔记3
date: 2019-04-28
updated: 2019-05-03
categories:
- 数学
tags:
- 读书笔记
- 概率论
permalink: probability-and-statistics-note-3
mathjax: true
---

* {% post_link probability-and-statistics-note-1 %}
* {% post_link probability-and-statistics-note-2 %}
* {% post_link probability-and-statistics-note-3 %}
----

# 第7章 伪随机数
$$\Huge{不要自己手动生成随机数！}$$

## 随机数序列
本书将 i.i.d. 随机变量序列 $X_1,X_2,\cdots$（或其对应的观测值）称为随机数序列或真随机数序列
而在不通过物理模拟的前提下，获得一种以假乱真的随机数序列替代品方法，称之为**伪随机数序列**

伪随机数序列 $x_1,x_2,x_3,\cdots$ 通常由以下方式生成：
$$s_{t+1}\equiv g(s_t),  x_t\equiv h(s_t),  (t=1,2,3,\cdots)$$
通过函数 $g$ 更新内部状态 $s_t$，根据内部状态确定函数 $h$ 并输出为 $x_t$
开始的状态 $s_1$ 被称为种子，为真随机数，相同的种子产生相同的伪随机数序列

## 梅森旋转算法
* 计算速度快
* 周期较长 $p = 2^{19937}-1$
* 当 $k$ 较大时，由连续 $k$ 个值形成的向量遵从均匀分布（例如 $k=623$ 时，连续623个值在623维的超立方体内以32位的精度均匀分布）

## 蒙特卡罗方法（Mente Carlo method）
* 蒙特卡洛方法是一种随机数序列的典型应用
* 经典问题：1/4 圆上进行模拟随机投掷飞镖，命中概率趋近 π/4
* 通过随机数序列，模拟计算期望值
* 不过收敛较慢，提升一位精度不得不增加100倍运算规模
* 当分布非常复杂，曲线不光滑或包含高维度积分时，才考虑使用蒙特卡罗方法方法——“百无所依时最后的救命稻草”

## 密码理论中的伪随机数序列
低差异序列（low-discrepancy sequence，一种取值分布非常均匀的序列），相比梅森旋转算法之类的，分布更加均匀

低差异序列也可以用于对蒙特卡罗方法进行改良，提高特定情况下的收敛速度，被称为拟蒙特卡罗算法

## 遵从离散值分布的随机数生成
**均匀分布：**

通过 $[0,1)$ 上均匀分布的随机变量 $X$，乘 n 取整再+1，就能获得1到n的各整数离散值的均匀分布。

**一般分布：**

采用各离散值的概率，堆叠在0到1的范围内，当随机变量 X 落在哪个范围，则命中哪个离散值

## 遵从连续值分布的随机数生成
**均匀分布：**

为了获得 a 与 b 之间均匀分布的随机数，只需要定义 $Y≡(b-a)X+a$

**非均匀分布，通过累积分布函数生成：**

定义 $Y≡F^{-1}(X)$ ，其中 $F^{-1}$ 表示累积分布函数的反函数，

**非均匀分布，通过概率密度函数生成：**

绘制概率密度函数 f 的图像，在能够框住整个图像的矩形范围内随机打点，如果点位于图像下方则接受该点，并返回横轴值，如果点位于图像上方则重新生成点

## 遵从正态分布的随机数的生成
**Box-Muller变换**

将均匀分布转换为正态分布，设 $X_1,X_2$ 是在[0,1)上遵从均匀分布的i.i.d.随机变量，定义：
$$Y_1 ≡ g(X_1, X_2) ≡ \sqrt{-2\log X_1}\cos(2πX_2)$$
$$Y_2 ≡ h(X_1, X_2) ≡ \sqrt{-2\log X_1}\sin(2πX_2)$$

则 $(Y_1,Y_2)^T$ 遵从二元正态分布，是两个独立的遵从正态分布的随机数

**均匀分布的加法（近似遵从正态分布）**

使用中心极限定理，设若干个在[0,1)上遵从均匀分布的i.i.d.随机变量 $X_1,X_2,⋯,X_{2n}$，设 $Y≡X_1+X_2+⋯+X_{2n}-n$，则 $Y$ 近似遵从正态分布

**遵从多元正态分布的随机数生成**

设 $Z_1,⋯,Z_n$ 是遵从标准正态分布 $\mathrm{N}(0,1)$ 的随机变量，由这些随机变量构成的向量 $\boldsymbol{Z}≡(Z_1,⋯,Z_n)^T$ 遵从n元标准正态分布

将其乘上一个n元正规矩阵后再加上一个n元向量，得到 $\boldsymbol{X}\equiv A\boldsymbol{Z}+\boldsymbol{\mu}$ 将遵从 $\mathrm{N}(\boldsymbol{\mu}, AA^T)$

也就是说如果想遵从特定的 $\mathrm{N}(\boldsymbol{\mu},V)$ ，只需要找到满足 $V=AA^T$ 的 $A$ 即可（可以利用Cholesky分解，它是LU分解的非负常量对称矩阵形式）

## 三角形内的均匀分布
例子：

* 设 $X_1, X_2$ 是在 [0,1) 上遵从均匀分布的i.i.d.随机变量
* 设 $(Y_1, Y_2) ≡ (min(X_1, X_2), |X_1 - X_2|)$ ，变成一个等腰直角三角形
* 设 $(Z_1, Z_2) ≡ (3Y_1+Y_2, Y_1+2Y_2)$ ，变成某一个任意形状三角形
* 设 $(W_1, W_2) ≡ (Z_1+7, Z_2+3)$，位移

也可以类似推广到高维情况

## 球面上的均匀分布
设 $Z_1, Z_2, Z_3$ 是遵从标准正态分布的i.i.d.随机变量，定义 $\boldsymbol{Z}\equiv(Z_1,\cdots,Z_3)^T$ 遵从3元标准正态分布
$$\boldsymbol{W}\equiv \frac{1}{\|\boldsymbol{Z}\|}\boldsymbol{Z}$$

即为标准球面上的均匀分布，因为正态分布各个方向分布相同

# 第8章 概率论的各类应用
## 8.1 回归分析与多变量分析
### 最小二乘法拟合直线
**线性回归问题**

一个误差遵从正态分布的试验项目，进行 $n$ 次试验，获得样本数据 $x_1,\cdots,x_n$ 和 $\check{y}_1,\dots,\check{y}_n$

假设 $g(x)\equiv ax+b$ ，其中 $a,b$ 为未知常量，设 $Y_i\equiv g(x)+W_i$ ，其中误差 $W_i\sim\mathrm{N}(0,\sigma^2)$，且 $W_1,\cdots,W_n$ 满足i.i.d.条件。

可以归纳为矩阵方式表示：
$$
\boldsymbol{Y}=C\boldsymbol{a}+\boldsymbol{W},
\boldsymbol{Y}\equiv\left(\begin{array}{c}Y_1\\\vdots\\Y_n\end{array}\right),
C\equiv\left(\begin{array}{cc}x_1&1\\\vdots&\vdots\\x_n&1\end{array}\right),
\boldsymbol{a}\equiv\left(\begin{array}{c}a\\b\end{array}\right),
\boldsymbol{W}\equiv\left(\begin{array}{c}W_1\\\vdots\\W_n\end{array}\right)
\sim\mathrm{N}(\boldsymbol{o}, \sigma^2I)
$$

推导得到 $\boldsymbol{Y}\sim\mathrm{N}(C\boldsymbol{a},\sigma^2I)$

概率密度函数形式如下：（其中 $\Box$ 为常量）
$$f_{\boldsymbol{Y}}(\boldsymbol{y})=\Box\exp(-\frac{1}{2\sigma^2}\|\boldsymbol{y}-C\boldsymbol{a}\|^2)$$

使用观测值 $\check{\boldsymbol{y}}\equiv(\check{y}_1,\cdots,\check{y}_n)^T$ 作为 $\boldsymbol{Y}$ 的观察值来估计 $\boldsymbol{a}$ 的值。采用**最大似然估计**，则使得 $f_{\boldsymbol{Y}}(\boldsymbol{y})$ 最大即可，也就是让 $\|\check{\boldsymbol{y}}-C\boldsymbol{a}\|^2$ 最小（即均方误差最小化）

记 $h(a,b)=\sum_{i=1}^n(\check{y}_i-(ax_i+b))^2$，为求其最小值，需满足：
$$\frac{\partial h}{\partial a}=0\quad and \quad \frac{\partial h}{\partial b}=0$$

最终可形成对于 $a,b$ 的一元方程组，称之为“正则方程组”，即可算出结果。其矩阵表述可推导为：
$$C^TC\boldsymbol{a}=C^T \boldsymbol{\check{y}}$$

以上推导过程基于认为误差遵从正态分布，实际应用中通常会不在意是否遵从正态分布，直接采用均方误差来最小化。

进一步从几何角度理解，由所有可能的 $C\boldsymbol{a}$ 构成的集合形成平面：
$$
\mathrm{lm}C=\mathrm{span}(\boldsymbol{x},\boldsymbol{u}),\qquad
\boldsymbol{x}\equiv\left(\begin{array}{c}x_1\\\vdots\\x_n\end{array}\right),\quad
\boldsymbol{u}\equiv\left(\begin{array}{c}1\\\vdots\\1\end{array}\right)
$$

那么求 $\|\boldsymbol{\check{y}}-C\boldsymbol{a}\|^2$ 最小值就相当于寻找 $\mathrm{lm}C$ 上与 $\boldsymbol{\check{y}}$ 最近的点，也就是垂足

### 吉洪诺夫正则化
为避免线性方程组病态问题，通常可以加入正则化，例如上文问题中，最小化值如下，即吉洪诺夫正则化：
$$\|\boldsymbol{\check{y}}-C\boldsymbol{a}\|^2+\alpha\|\boldsymbol{a}\|^2\qquad(\alpha >0)$$

上文问题的基于贝叶斯估计的解法：
* 设 $\boldsymbol{A}$ 为遵从 $m$ 元正态分布 $\mathrm{N}(o,\tau^2I)$ 的向量值随机变量
* 对于已知 $n\times m$ 矩阵 $C$，得到 $\boldsymbol{Y}\equiv C\boldsymbol{A}+\boldsymbol{W}$，其中噪声 $\boldsymbol{W}\sim\mathrm{N}(o,\sigma^2I)$，且与 $\boldsymbol{A}$ 独立
* 观测 $\boldsymbol{Y}$ 的值得到 $\boldsymbol{\check{y}}$，可以采用两种方法估计 $\boldsymbol{A}$ ：
  * 计算让条件概率密度 $f_{\boldsymbol{A}|\boldsymbol{Y}}(\boldsymbol{a}|\boldsymbol{\check{y}})$ 取值最大的 $\boldsymbol{a}$
  * 计算条件期望值 $\mathrm{E}[\boldsymbol{A}|\boldsymbol{Y}=\boldsymbol{\check{y}}]$，将其作为 $\boldsymbol{A}$ 的估计值

该方法计算结果，与 $\alpha=\sigma^2/\tau^2$ 时的吉洪诺夫正则化结果一致（日文版草稿上有详细推导）

### 主成分分析
现实中的高维数据通常不会在所有维度都均匀分布，而是沿着一些特定方向散布，主成分分析（PCA）就是找出特定方向，降低维度

**主成分分析算法**

假设有 $n$ 条高维向量数据 $\boldsymbol{x}_1,\cdots,\boldsymbol{x}_n$ ，借助一个取值范围 $1,2,\cdots,n$ 且各值出现概率相等的转盘来生成编号$J$，并定义随机变量 $\boldsymbol{X}\equiv\boldsymbol{x}_J$

此处假设期望值 $E[\boldsymbol{X}]=\boldsymbol{o}$ （即平均值为零）。可通过高维椭圆来表现协方差矩阵 $V[\boldsymbol{X}]$，如果数据沿着某一特定方向散布，则该高维椭圆中长度较长的主轴很少，大部分主轴非常短，主成分分析将沿着较短主轴方向压缩椭圆

参照前文对一般协方差矩阵计算椭圆的方法，各主轴半径为 $V[\boldsymbol{X}]$ 的特征值 $\lambda_1,\lambda_2,\cdots$ 的平方根，且主轴方向与对应特征向量 $\boldsymbol{q}_1,\boldsymbol{q}_2,\cdots$ 方向一致。将特征值从大到小排序，并保证特征向量为单位向量且相互正交，则称得到的 $\boldsymbol{q}_i$ 为第 $i$ 主成分向量

主成分分析则会仅保留前 $k$ 个特征值与特征向量而舍弃后面的。其中主成分个数 $k$ 的确定方式，可以根据 $\lambda_i$ 阈值或大幅度缩小，或者贡献率来决定

选中的主成分会形成一个由 $\boldsymbol{q}_1,\cdots,\boldsymbol{q}_k$ 张成的 $k$ 维超平面 $\Pi$，将原始向量 $\boldsymbol{x}$ 正交投影至 $\Pi$，则完成维度压缩：（$z_i$称为$\boldsymbol{x}$的第$i$主成分）
$$\boldsymbol{y}=z_1\boldsymbol{q}_1+z_2\boldsymbol{q}_2+\cdots+z_k\boldsymbol{q}_k$$
$$z_i=\boldsymbol{q}_i\cdot\boldsymbol{x}=\boldsymbol{q}_i^T\boldsymbol{x}$$

矩阵表示为：
$$\boldsymbol{z}=R^T\boldsymbol{x},\qquad R\equiv(\boldsymbol{q}_1,\cdots,\boldsymbol{q}_k)$$
$$\boldsymbol{y}=R\boldsymbol{z}=RR^T\boldsymbol{x}$$

**保留成分个数**

原始数据 $\boldsymbol{x}$ 与由压缩数据 $\boldsymbol{z}$ 还原的 $\boldsymbol{y}$ 之间差值 $\boldsymbol{x}-\boldsymbol{y}$ 相当于被舍弃部分，定义随机变量 $\boldsymbol{Y}\equiv RR^T\boldsymbol{X}$，有：
$$\mathrm{E}[\|\boldsymbol{X}-\boldsymbol{Y}\|^2]=\lambda_{k+1}+\cdots+\lambda_m$$
$$\mathrm{E}[\|\boldsymbol{X}\|^2]=\mathrm{Tr}\ \mathrm{V}[\boldsymbol{X}]=\lambda_1+\cdots+\lambda_m$$
记 $\frac{\mathrm{E}[\|\boldsymbol{X}-\boldsymbol{Y}\|^2]}{\mathrm{E}[\|\boldsymbol{X}\|^2]}$ 为前 $k$ 个主成分的累积贡献率。通过对其限定一个百分比，从而确定 $k$

**平均值不为零的数据PCA处理**

给定高维向量数据 $\boldsymbol{x}_1,\cdots,\boldsymbol{x}_n$ 平均值不为零的话，则统一减去平均值向量，然后做PCA即可

**注意事项**

PCA处理的数据各维度单位要统一，因为各维度进行缩放时，原本的各主轴将不再两两正交，将产生新的主轴和PCA结果

## 8.2 随机过程（stochastic process）
一些随着时间随机变化的序列，可以视作随机变量序列，即随机过程
### 随机游走（random walk）
例如随机抛硬币，每次如果正面向上则向左走一步，否则向右走一步

随机游走可以有衍生版本，比如概率不同、多个方向等，但以下仅考虑一元等概率左右移动的随机游走

设 $Z_1,Z_2,\cdots,$ 是i.i.d.随机变量，且取值为+1或-1概率各为0.5，则有：
$$X_0=0, \qquad X_t=X_{t-1}+Z_t\quad(t=1,2,\cdots)$$

**习题1**

抛20次硬币，正面加一，反面减一，最终恰好为10。

即求 $X_20=10$ 的概率，明显应当是20次中15次向上，5次向下，则采用二项分布 $\mathrm{Bn}(20,1/2)$ 在 $15$ 位置的概率。

**习题2**

抛20次硬币，中途至少获得过5，但最终归为0的概率。即 $\mathrm{P}(X_{20}=0且之前出现过5)$

以最后一次到达5位置的时刻 $T$，对后续路线进行翻转操作（反射原理，reflection principle），则相当于最终到达10位置。结果和上题结果一致。

**习题3**

到达10时停止，求恰好是20次的概率。
$$\mathrm{P}(t=20时X_t=10首次成立)=\mathrm{P}(X_{19}=9且之前没有出现过10)*\mathrm{P}(Z_{20}=+1)$$
$$\mathrm{P}(X_{19}=9且之前没有出现过10)=P(X_{19}=9)-P(X_{19}=9且之前出现过10)$$

**其他**

当 $n\to\infty$ 时，游戏过程中手头金额为正数的时间比例小于 $\alpha$ 的概率趋向于 $(2/\pi)\arcsin\sqrt{\alpha}$ （反正弦定律），也就可以算出来盈利时间比超过99.5%的极端情况发生概率高达4.5%左右

盈亏逆转的概率也极低，随着n变大，逆转次数提升非常少。但这并不影响再已经盈利或亏损情况下再次下注的情况，因为条件概率下：
$$\mathrm{E}[X_j|X_i=a]=a\qquad (j>i)$$

博弈论中“鞅”（martingale）模型即用于描述该性质

### 卡尔曼滤波器
针对随机过程中 $t$ 时刻的实数值随机变量 $X_t$，考虑误差 $Z_t$，则有测定值 $Y_t=X_t+Z_t$。定义两次位置变化量为 $W_t$，则有 $X_t=X_{t-1}+W_t$ 。假定 $X_0\sim\mathrm{N}(0,\sigma^2_0),W_t\sim\mathrm{N}(0,\alpha^2),Z_t\sim\mathrm{N}(0,\beta^2)$，所有 $X_0,W_i,Z_i$ 均相互独立。试图通过 $Y_1,\cdots,Y_t$ 来估计 $X_t$ ：

**计算过程**

考虑 $t=1$ 情况，表示为矩阵形式为：
$$\left(\begin{matrix} Y_1\\X_1 \end{matrix}\right)=
J\left(\begin{matrix}Z_1\\W_1\\X_0\end{matrix}\right),\qquad
J=\left(\begin{matrix}1&1&1\\0&1&1\end{matrix}\right)$$

由于 $(Z_1,W_1,X_0)^T\sim\mathrm{N}(\boldsymbol{o},\mathrm{diag}(\beta^2,\alpha^2,\sigma^2_0))$ ，则通过 $J$ 投影后的 $(Y_1,X_1)^T$ 将遵从二元正态分布：

$$\left(\begin{matrix} Y_1\\X_1 \end{matrix}\right)\sim\mathrm{N}(\boldsymbol{o},V_1)$$
$$V_1\equiv J \left(\begin{matrix} \beta^2&0&0\\0&\alpha^2&0\\0&0&\sigma^2_0 \end{matrix}\right) J^T = 
\left(\begin{matrix}\tau^2_1+\beta^2&\tau^2_1\\\tau^2_1&\tau^2_1 \end{matrix}\right),\qquad
\tau^2_1\equiv \mathrm{V}[X_1]=\sigma^2_0+\alpha^2$$

则可推导出当 $Y_1=y_1$ 时 $X_1$ 的条件分布 $\mathrm{N}(\mu_1,\sigma^2_1)$ （根据多元正态分布的条件分布计算方法）：
$$\mu_1\equiv \mathrm{E}[X_1|Y_1=y_1]=\frac{\tau^2_1y_1}{\tau^2_1+\beta^2}$$
$$\sigma^2_1\equiv \mathrm{V}[X_1|Y_1=y_1]=\frac{\tau^2_1\beta^2}{\tau^2_1+\beta^2}$$

推广到一般的 $t$ 情况，矩阵形式为：
$$\left(\begin{matrix} Y_t\\X_t \end{matrix}\right)=
J\left(\begin{matrix}Z_t\\W_t\\X_{t-1}\end{matrix}\right),\qquad
J=\left(\begin{matrix}1&1&1\\0&1&1\end{matrix}\right)$$

基于条件 $Y_{t_1}=y_t,\cdots,Y_1=y_1$，有 $(Z_t,W_2,X_{t-1})^T\sim\mathrm{N}((0,0,\mu_{t-1})^T,\mathrm{diag}(\beta^2,\alpha^2,\sigma^2_{t-1}))$，有：
$$\tau^2_t\equiv \mathrm{V}[X_t|Y_{t-1}=y_{t-1},\cdots,Y_1=y_1]=\sigma^2_{t-1}+\alpha^2$$
增加一个条件 $Y_t=y_t$ ，可知 $X$ 的条件分布  $\mathrm{N}(\mu_t,\sigma^2_t)$：
$$\mu_t\equiv \mathrm{E}[X_t|Y_t=y_t,\cdots,Y_1=y_1]=\frac{\tau^2_ty_t+\beta^2\mu_{t-1}}{\tau^2_t+\beta^2}$$
$$\sigma^2_t\equiv \mathrm{V}[X_t|Y_t=y_t,\cdots,Y_1=y_1]=\frac{\tau^2_t\beta^2}{\tau^2_t+\beta^2}$$

对此的形象化表述为：目标的上一个位置 $\mu_{t-1}$ 由之前数据估计得到（误差为 $\sigma^2_{t-1}$），那么本次位置应该仍然在 $\mu_{t-1}$ 附近，但误差增大到 $\tau^2_t=\sigma^2_{t-1}+\alpha^2$，然而根据本次数据，当前位置也应该在 $y_t$ 附近（误差为 $\beta^2$），因此以他们的误差作为权重，对两个位置进行加权平均

**形式化表述**
$$
\begin{aligned}
f&(y_t,x_t|y_{t_1},\cdots,y_1)\\
&=\int_{-\infty}^{\infty}f(y_t,x_t,x_{t-1}|y_{t-1},\cdots,y_1)\mathrm{d}x_{t-1}\\
&=\int_{-\infty}^{\infty}f(y_t|x_t,x_{t-1},y_{t-1},\cdots,y_1)f(x_t|x_{t-1},y_{t-1},\cdots,y_1)f(x_{t-1}|y_{t-1},\cdots,y_1)\mathrm{d}x_{t-1}\\
&=\int_{-\infty}^{\infty}f(y_t|x_t)f(x_t|x_{t-1})f(x_{t-1}|y_{t-1},\cdots,y_1)\mathrm{d}x_{t-1}\\
&=\int_{-\infty}^{\infty}g(y_t;x_t,\beta^2)g(x_t;x_{t-1},\alpha^2)g(x_{t-1};\mu_{t-1},\sigma_{t-1}^2)\mathrm{d}x_{t-1}\\
\end{aligned}
$$

其中 $g(x;\mu,\sigma^2)$ 表示 $\mathrm{N}(\mu,\sigma^2)$ 的概率密度函数

**进阶**

不仅可以用于估计当前位置的值，也可以预测下一位置的值：在 $Y_1=y_1,\cdots,Y_{t-1}=y_{t-1}$ 条件下，$X_t$ 的条件分布为 $\mathrm{N}(\mu_{t-1},\tau^2_t)$

卡尔曼滤波器更多适用于高维情况，有对应的高维向量推导。另外不仅可以用于位置不会突然改变情况，也可以用于速度不会突然改变情况。

### 马尔可夫链
前面的随机游走、卡尔曼滤波等问题，都不关心之前路径，而只用当前位置来预测将来位置，这类随机过程统称为**马尔可夫过程**（Markov process）。如果马尔可夫过程中 $X_t$ 的取值范围优先，则可称之为**马尔可夫链**（Markov chain）

**（离散时间有限状态的平稳）马尔可夫链：**

随机变量序列 $X_0,X_1,X_2,\cdots$，且 $X_t$ 的取值范围为 $1,2,\cdots,n$ 。定义 $n$ 个状态，随着时间 $t$ 的变化，状态将不断转移。定义转移概率 $\mathrm{P}(X_{t+1}=i|X_t=j)$ （仅讨论时间平稳情况，即与时间无关），记为 $p_{i\gets j}$

**转移概率矩阵**

记 $P$ 为转移概率矩阵，其每列数字之和全都为1，但各行不一定
$$P\equiv\begin{pmatrix}
p_{1\gets 1}&\cdots&p_{1\gets n}\\
\vdots &&\vdots\\
p_{n\gets 1}&\cdots&p_{n\gets n}\\
\end{pmatrix} $$

将 $X_t$ 的分布表示为向量 $\boldsymbol{u}_t$：
$$\boldsymbol{u}_t\equiv\begin{pmatrix}
\mathrm{P}(X_t=1)\\
\vdots\\
\mathrm{P}(X_t=n)\\
\end{pmatrix}$$

初始分布和转移概率矩阵决定了马尔可夫链的所有分布，即：
$$\boldsymbol{u}_{t+1}=P\boldsymbol{u}_t,\qquad\boldsymbol{u}_t=P^t\boldsymbol{u}_0$$

对于条件分布，例如状态个数 $n=3$，而 $X_t=2$ 时的条件分布：
$$\begin{pmatrix}
P(X_{t+k}=1|X_t=2)\\
P(X_{t+k}=2|X_t=2)\\
P(X_{t+k}=3|X_t=2)\\
\end{pmatrix}=P^k
\begin{pmatrix}
0\\1\\0
\end{pmatrix}$$

**平稳分布**

满足 $P\boldsymbol{u}_0=\boldsymbol{u}_0$，则分布不会随着时间变化，保持不变，称之为平稳分布。任何转移概率矩阵 $P$ 都存在对应的平稳分布，可以通过列方程组，满足  $P\boldsymbol{u}_0=\boldsymbol{u}_0$ 以及每列数字之和为1条件，求出平稳分布

**极限分布**

对于很多状态概率矩阵 $P$，无论初始分布 $\boldsymbol{u}_0$ 如何，经过一定时间总能收敛到一个平稳分布 $\boldsymbol{u}_t$，则该分布 $\boldsymbol{u}$ 称之为极限分布

$$\lim_{t\to\infty}P^t=\Bigg(\boldsymbol{u}\Bigg|\cdots\Bigg|\boldsymbol{u}\Bigg)=\boldsymbol{u}\boldsymbol{1}^T, \quad\boldsymbol{1}^T\equiv(1,\cdots,1)$$
$$\lim_{t\to\infty}P^t\boldsymbol{u}_0=(\boldsymbol{u}\boldsymbol{1}^T)\boldsymbol{u}_0=\boldsymbol{u}$$

设 $\boldsymbol{u}\equiv(u_1,\cdots,u_n)^T$，各状态 $i$ 所占时间比例将收敛至 $u_i$，即：
$$\lim_{t\to\infty}\frac{X_1,\cdots,X_t中值为i的变量个数}{t} = u_i$$

* 左式为人类视角：随着时间变化，持续观察序列，其中状态 $i$ 的出现比例
* 右式为上帝视角：在时间 $t$，横跨各平行世界观察

有一些情况下极限分布不存在，包括不同初始分布会收敛到不同的平稳分布中，或者转移存在周期性，或其他不收敛情况

**吸收概率**

马尔可夫链之中有一些状态进入后即永远循环在该状态，那么从其他某个状态为起点，落入该状态的概率称之为吸收概率。由于无需考虑历史情况，可以通过联立方程组，列举各起点直接落入某循环状态和转移到其他节点后再落入某循环状态的概率，从而解出方程

**首次到达时刻**

在马尔可夫链中约定某状态为起点，某状态为终点，计算从起点到重点所需转移次数的期望值。仍然可以通过联立方程组，列举各状态到达终点的期望值之间的关系，从而解出方程

**隐马尔科夫模型（HMM）**

举例语音识别技术，简化版本为：定义字符串 $X_0,X_1,\cdots$ 来表示人所说的话（正确答案），而只能观察到辅音部分，记为 $Y_0,Y_1,\cdots$。

为 $X_t$ 构造马尔可夫链：统计各音节位于句首的概率，各音节紧跟在其他音节之后的概率等。最终通过观察到的 $Y_t$ 来估计 $X_t$

为加快速度，有Viterbi算法，使用动态规划来计算条件概率最大的音节序列。也会借助Forward-Backword算法（Baum-Welch算法）来估计转移概率矩阵

## 8.3 信息论
### 熵
熵表示信息量的大小，可以理解为得到该消息的意外程度（如果采用 $\log_2$，则熵的单位是bit）。当得知 $X=x$ 时的意外程度为：
$$h(x)\equiv\log\frac{1}{\mathrm{P}(X=x)}$$

对于离散值随机变量 $X$ 的分布的熵记为 $\mathrm{H}[X]$，则：
$$\mathrm{H}[X]\equiv\mathrm{E}[h(X)]=\sum_{x}\mathrm{P}(X=x)\log\frac{1}{\mathrm{P}(X=x)}$$

如果 $X$ 的可能取值有 $m$ 种，则当遵从均匀分布时熵最大，为 $\mathrm{H}[X]=\log m$

对于实数值随机变量 $X$ 的分布的熵，可计算为：
$$\mathrm{H}[X]\equiv\mathrm{E}[h(X)]=\int f_X(x)\log\frac{1}{f_X(x)}\mathrm{d}x\quad（积分范围为f_X(x)不为零的全部部分）$$

### 二元熵
对于联合分布，当得知 $X=x,Y=y$ 时的意外程度为：
$$h(x,y)\equiv\log\frac{1}{\mathrm{P}(X=x,Y=y)}$$

其期望值称为**联合熵**：
$$\mathrm{H}[X,Y]\equiv\mathrm{E}[h(X,Y)]=\sum_x\sum_y\mathrm{P}(X=x,Y=y)\log\frac{1}{\mathrm{P}(X=x,Y=y)}$$

当已知 $X=x$ 情况下，得知 $Y=y$ 的意外程度为：
$$h(y|x)\equiv\log\frac{1}{\mathrm{P}(Y=y|X=y)}$$

其期望值称为**条件熵**：
$$\mathrm{H}[Y|X]\equiv\mathrm{E}[h(Y|X)]=\sum_x\sum_y\mathrm{P}(X=x,Y=y)\log\frac{1}{\mathrm{P}(Y=y|X=x)}$$

联合熵与条件熵具有性质：
$$\mathrm{H}[X,Y]=\mathrm{H}[Y|X]+\mathrm{H}[X]=\mathrm{H}[X|Y]+\mathrm{H}[Y]$$

**互信息**，没有条件情况下得知 $X$ 与没有条件情况下得知 $Y$ 的信息量差异，如果差异越大，说明相互包含信息越多：
$$\mathrm{I}[X;Y]\equiv\mathrm{H}(Y)-\mathrm{H}[Y|X]=\mathrm{H}(X)-\mathrm{H}[X|Y]$$

$$\mathrm{I}[X;Y]=0\quad\Leftrightarrow\quad X与Y独立 \quad\Rightarrow\quad\rho_{X,Y}=0$$

联合熵、条件熵、互信息的关系：
$$
\begin{aligned}
&|\Leftarrow && \mathrm{H}[X,Y] && \qquad\Rightarrow&|\\
&|\Leftarrow \qquad\mathrm{H}[X] && \Rightarrow&|&\Leftarrow\quad\mathrm{H}[Y|X]\Rightarrow&|\\
&|\Leftarrow\mathrm{H}[X|Y]\Rightarrow&|&\Leftarrow && \mathrm{H}[Y]\qquad\Rightarrow &|\\
&| &|& \Leftarrow\mathrm{I}[X;Y]\Rightarrow &|& &|\\
\end{aligned}
$$

### 信源编码
例如对字符串进行压缩，原始字符串 $X$ 长度为 $n$，由 $k$ 种不同的字符 $a_1,\cdots,a_k$ 组成。目标是压缩为长度为 $m$ 的比特序列 $Y$。要进行压缩，必然 $k^n>2^m$。研究问题是对于给定的一段字符串，最大可以压缩到多小：

* 当压缩率 $m/n<\mathrm{H}[X_i]$，只要n足够大，压缩成功率将非常接近于1
* 当压缩率 $m/n>\mathrm{H}[X_i]$，只要n足够大，压缩成功率将非常接近于0

### 信道编码
如果通信线路或者内存设备质量较差，可能出现错误，则需要加入一定冗余来增强信息的正确性，例如校验和

举例：用随机变量 $X$ 表示发送的字符，用随机变量 $Y$ 表示接收的字符，则信道特性可以通过条件分布 $\mathrm{P}(Y=y|X=y)$ 表示

需要做的是调节 $X$ 的分布，使发送信息和接收信息的互信息 $\mathrm{I}(X,Y)$ 尽可能大，其最大上限 $c$ 称为信道容量。定义信道传输速率 $r$ 指实际传输1个字符所需发送比特数，可以理解为编码前后的字符串长度比
* 如果信息传输速率 $r<c$，必然存在一种信道编码方式使得通信错误率达到任意小的值
* 如果 $r>c$，则无法将通信错误率降低到任意小的值

# 附录
**无限集的大小**
* 能够与自然数集 $\mathbb{N}$ 元素一一对应的集合称为**可数集**，其集合的势为 $\aleph_1$
* 整数集 $\mathbb{Z}$、有理数集 $\mathbb{Q}$ 均为可数集
* 实数集 $\mathbb{R}$ 不是可数集，其集合的势为 $\aleph_2$

**高斯积分**
$$\int_{-\infty}^{\infty}\exp(-ax^2)\mathrm{d}x=\sqrt\frac{\pi}{a}$$

**内积**

假设某个内积空间的两个标准正交基
$$
\boldsymbol{x}=\begin{pmatrix} x_1\\\vdots\\x_n \end{pmatrix},\qquad
\boldsymbol{y}=\begin{pmatrix} y_1\\\vdots\\y_n \end{pmatrix}
$$
内积计算：
$$\boldsymbol{x}\cdot\boldsymbol{y}=\boldsymbol{y}\cdot\boldsymbol{x}=\boldsymbol{x}^T\boldsymbol{y}=\boldsymbol{y}^T\boldsymbol{x}$$
$$\boldsymbol{x}\cdot\boldsymbol{y}=\|\boldsymbol{x}\|\|\boldsymbol{y}\|\cos\theta$$
$$\|\boldsymbol{x}\|^2=\boldsymbol{x}\cdot\boldsymbol{x}=\boldsymbol{x}^T\boldsymbol{x}$$

**琴生不等式、吉布斯不等式、马尔可夫不等式、切比雪夫不等式、切尔诺夫界、民科夫斯基不等式、赫尔德不等式**(略)

**特征函数**

实数值随机变量 $X$，定义特征函数为：
$$\phi_X(t)\equiv\mathrm{E}[e^{\mathrm{i}tX}]=\int_{-\infty}^{\infty}f_X(x)e^{\mathrm{i}tx}\mathrm{d}x$$

**KL散度与大偏差原理**（略）